"""
æ–°é—»è·å–å·¥å…· - æ”¯æŒä¸­æ–‡ç¿»è¯‘
"""
from langchain.tools import BaseTool
from typing import List, Dict, Any, Optional
import requests
from datetime import datetime, timedelta
from ..news_ingestor import ingest_once
from ..state_store import read_latest_news

class NewsTranslator:
    """æ–°é—»ç¿»è¯‘å™¨ - ç®€å•çš„å…³é”®è¯ç¿»è¯‘"""
    
    def __init__(self):
        self.finance_terms = {
            # å…¬å¸ç›¸å…³
            'earnings': 'è´¢æŠ¥', 'revenue': 'è¥æ”¶', 'profit': 'åˆ©æ¶¦', 'loss': 'äºæŸ',
            'quarterly': 'å­£åº¦', 'annual': 'å¹´åº¦', 'guidance': 'ä¸šç»©æŒ‡å¼•',
            'dividend': 'è‚¡æ¯', 'buyback': 'å›è´­', 'merger': 'å¹¶è´­', 'acquisition': 'æ”¶è´­',
            
            # å¸‚åœºç›¸å…³
            'stock': 'è‚¡ç¥¨', 'shares': 'è‚¡ä»½', 'market': 'å¸‚åœº', 'trading': 'äº¤æ˜“',
            'bull market': 'ç‰›å¸‚', 'bear market': 'ç†Šå¸‚', 'rally': 'ä¸Šæ¶¨', 'decline': 'ä¸‹è·Œ',
            'volatility': 'æ³¢åŠ¨', 'volume': 'æˆäº¤é‡',
            
            # è´¢åŠ¡æŒ‡æ ‡
            'EPS': 'æ¯è‚¡æ”¶ç›Š', 'P/E': 'å¸‚ç›ˆç‡', 'ROE': 'å‡€èµ„äº§æ”¶ç›Šç‡',
            'cash flow': 'ç°é‡‘æµ', 'debt': 'å€ºåŠ¡', 'assets': 'èµ„äº§',
            
            # è¡Œä¸šç›¸å…³
            'technology': 'ç§‘æŠ€', 'semiconductor': 'åŠå¯¼ä½“', 'AI': 'äººå·¥æ™ºèƒ½',
            'cloud': 'äº‘è®¡ç®—', 'software': 'è½¯ä»¶', 'hardware': 'ç¡¬ä»¶',
            'automotive': 'æ±½è½¦', 'electric vehicle': 'ç”µåŠ¨æ±½è½¦', 'EV': 'ç”µåŠ¨æ±½è½¦',
            'healthcare': 'åŒ»ç–—', 'pharmaceutical': 'åˆ¶è¯', 'biotech': 'ç”Ÿç‰©æŠ€æœ¯',
            
            # åŠ¨ä½œè¯
            'announces': 'å®£å¸ƒ', 'reports': 'å‘å¸ƒ', 'launches': 'æ¨å‡º',
            'increases': 'å¢åŠ ', 'decreases': 'å‡å°‘', 'beats': 'è¶…è¶Š',
            'misses': 'æœªè¾¾', 'expects': 'é¢„æœŸ', 'forecasts': 'é¢„æµ‹'
        }
    
    def translate_title(self, title: str) -> str:
        """ç¿»è¯‘æ ‡é¢˜ä¸­çš„å…³é”®è´¢ç»æœ¯è¯­"""
        translated = title
        for en_term, cn_term in self.finance_terms.items():
            # ç®€å•æ›¿æ¢ï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç¿»è¯‘API
            translated = translated.replace(en_term, f"{en_term}({cn_term})")
        return translated
    
    def translate_summary(self, summary: str) -> str:
        """ç¿»è¯‘æ‘˜è¦"""
        if not summary or len(summary) < 10:
            return summary
            
        # ç®€å•çš„å…³é”®è¯æ›¿æ¢
        translated = summary
        for en_term, cn_term in self.finance_terms.items():
            translated = translated.replace(en_term, f"{en_term}({cn_term})")
        
        return translated[:300] + "..." if len(translated) > 300 else translated

class NewsRetrievalTool(BaseTool):
    name = "get_financial_news"
    description = "è·å–æœ€æ–°çš„è´¢ç»æ–°é—»ã€‚è¾“å…¥ï¼šhours (è·å–æœ€è¿‘Nå°æ—¶çš„æ–°é—»ï¼Œé»˜è®¤24), limit (æœ€å¤§æ¡æ•°ï¼Œé»˜è®¤10), keywords (å¯é€‰çš„å…³é”®è¯è¿‡æ»¤)ã€‚è¾“å‡ºï¼šä¸­æ–‡ç¿»è¯‘çš„æ–°é—»æ‘˜è¦ã€‚"

    def __init__(self):
        super().__init__()
        # ä½¿ç”¨ object.__setattr__ ç»•è¿‡ Pydantic é™åˆ¶
        object.__setattr__(self, 'translator', NewsTranslator())

    def _run(self, hours: int = 24, limit: int = 10, keywords: str = None) -> str:
        try:
            # 1. å…ˆå°è¯•ä»æœ¬åœ°è·å–æ–°é—»
            news_items = read_latest_news(limit=limit * 2)  # å¤šè·å–ä¸€äº›ï¼Œä¾¿äºè¿‡æ»¤
            
            # 2. å¦‚æœæœ¬åœ°æ–°é—»ä¸å¤Ÿæ–°ï¼Œå…ˆæŠ“å–ä¸€æ¬¡
            if not news_items or self._is_news_stale(news_items[0], hours):
                print("æœ¬åœ°æ–°é—»è¿‡æœŸï¼Œæ­£åœ¨æŠ“å–æœ€æ–°æ–°é—»...")
                ingest_once()  # æŠ“å–æœ€æ–°æ–°é—»
                news_items = read_latest_news(limit=limit * 2)
            
            if not news_items:
                return "æš‚æ—¶æ— æ³•è·å–æ–°é—»ï¼Œè¯·ç¨åå†è¯•ã€‚"
            
            # 3. è¿‡æ»¤æ—¶é—´èŒƒå›´
            cutoff_time = datetime.utcnow() - timedelta(hours=hours)
            recent_news = []
            
            for item in news_items:
                try:
                    pub_time = datetime.fromisoformat(item['published_at'].replace('Z', '+00:00'))
                    if pub_time.replace(tzinfo=None) > cutoff_time:
                        recent_news.append(item)
                except:
                    continue  # è·³è¿‡æ—¶é—´è§£æå¤±è´¥çš„æ–°é—»
            
            if not recent_news:
                return f"æœ€è¿‘ {hours} å°æ—¶å†…æš‚æ— æ–°é—»æ›´æ–°ã€‚"
            
            # 4. å…³é”®è¯è¿‡æ»¤
            if keywords:
                filtered_news = self._filter_by_keywords(recent_news, keywords)
                if filtered_news:
                    recent_news = filtered_news
                else:
                    return f"æœ€è¿‘ {hours} å°æ—¶å†…æ²¡æœ‰åŒ…å« '{keywords}' çš„ç›¸å…³æ–°é—»ã€‚"
            
            # 5. é™åˆ¶æ•°é‡å¹¶ç¿»è¯‘
            recent_news = recent_news[:limit]
            return self._format_news_response(recent_news, hours, keywords)
            
        except Exception as e:
            return f"è·å–æ–°é—»æ—¶å‡ºé”™ï¼š{str(e)}"

    def _is_news_stale(self, latest_news: Dict, max_hours: int) -> bool:
        """æ£€æŸ¥æ–°é—»æ˜¯å¦è¿‡æœŸ"""
        try:
            pub_time = datetime.fromisoformat(latest_news['published_at'].replace('Z', '+00:00'))
            age_hours = (datetime.utcnow() - pub_time.replace(tzinfo=None)).total_seconds() / 3600
            return age_hours > max_hours
        except:
            return True

    def _filter_by_keywords(self, news_items: List[Dict], keywords: str) -> List[Dict]:
        """æ ¹æ®å…³é”®è¯è¿‡æ»¤æ–°é—»"""
        keywords_list = [kw.strip().lower() for kw in keywords.split(',')]
        filtered = []
        
        for item in news_items:
            title_lower = item.get('title', '').lower()
            summary_lower = item.get('summary', '').lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä¸€å…³é”®è¯
            if any(kw in title_lower or kw in summary_lower for kw in keywords_list):
                filtered.append(item)
        
        return filtered

    def _format_news_response(self, news_items: List[Dict], hours: int, keywords: Optional[str]) -> str:
        """æ ¼å¼åŒ–æ–°é—»å“åº”"""
        header_parts = [f"ğŸ“° æœ€è¿‘ {hours} å°æ—¶è´¢ç»æ–°é—»"]
        if keywords:
            header_parts.append(f"(å…³é”®è¯: {keywords})")
        header_parts.append(f"(å…± {len(news_items)} æ¡)\n")
        
        lines = ["".join(header_parts)]
        
        for i, item in enumerate(news_items, 1):
            # ç¿»è¯‘æ ‡é¢˜å’Œæ‘˜è¦
            title = self.translator.translate_title(item.get('title', ''))
            summary = self.translator.translate_summary(item.get('summary', ''))
            source = item.get('source', 'unknown')
            
            # æ ¼å¼åŒ–æ—¶é—´
            try:
                pub_time = datetime.fromisoformat(item['published_at'].replace('Z', '+00:00'))
                time_str = pub_time.strftime('%m-%d %H:%M')
            except:
                time_str = 'æ—¶é—´æœªçŸ¥'
            
            lines.append(f"{i}. ã€{source.upper()}ã€‘{title}")
            if summary and len(summary.strip()) > 10:
                lines.append(f"   ğŸ’¡ {summary}")
            lines.append(f"   ğŸ•’ {time_str}")
            lines.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(lines)

    def _arun(self, **kwargs):
        raise NotImplementedError("å¼‚æ­¥æš‚ä¸æ”¯æŒ")


class MarketNewsAnalysisTool(BaseTool):
    name = "analyze_market_sentiment"
    description = "åˆ†ææœ€è¿‘æ–°é—»çš„å¸‚åœºæƒ…ç»ªã€‚è¾“å…¥ï¼šhours (åˆ†ææœ€è¿‘Nå°æ—¶ï¼Œé»˜è®¤24), focus (å…³æ³¨é¢†åŸŸï¼Œå¦‚'tech'ã€'ai'ã€'ev'ç­‰)ã€‚è¾“å‡ºï¼šå¸‚åœºæƒ…ç»ªåˆ†æå’ŒæŠ•èµ„å»ºè®®ã€‚"

    def __init__(self):
        super().__init__()
        # ä½¿ç”¨ object.__setattr__ ç»•è¿‡ Pydantic é™åˆ¶
        object.__setattr__(self, 'translator', NewsTranslator())

    def _run(self, hours: int = 24, focus: str = None) -> str:
        try:
            # è·å–æ–°é—»
            news_tool = NewsRetrievalTool()
            keywords = self._get_focus_keywords(focus) if focus else None
            
            # è·å–åŸå§‹æ–°é—»æ•°æ®
            news_items = read_latest_news(limit=50)
            if not news_items:
                return "æ— æ³•è·å–æ–°é—»æ•°æ®è¿›è¡Œæƒ…ç»ªåˆ†æã€‚"
            
            # ç®€å•çš„æƒ…ç»ªåˆ†æ
            sentiment_analysis = self._analyze_sentiment(news_items, hours, focus)
            return self._format_sentiment_response(sentiment_analysis, hours, focus)
            
        except Exception as e:
            return f"å¸‚åœºæƒ…ç»ªåˆ†æå‡ºé”™ï¼š{str(e)}"

    def _get_focus_keywords(self, focus: str) -> str:
        """æ ¹æ®å…³æ³¨é¢†åŸŸè·å–å…³é”®è¯"""
        focus_map = {
            'tech': 'technology,software,cloud,AI,semiconductor',
            'ai': 'AI,artificial intelligence,machine learning,ChatGPT,OpenAI',
            'ev': 'electric vehicle,EV,Tesla,battery,automotive',
            'crypto': 'bitcoin,cryptocurrency,blockchain,crypto',
            'healthcare': 'healthcare,pharmaceutical,biotech,drug,medical'
        }
        return focus_map.get(focus.lower(), focus)

    def _analyze_sentiment(self, news_items: List[Dict], hours: int, focus: Optional[str]) -> Dict:
        """ç®€å•çš„æƒ…ç»ªåˆ†æ"""
        positive_words = ['beats', 'exceeds', 'strong', 'growth', 'profit', 'gains', 'rises', 'up']
        negative_words = ['misses', 'falls', 'decline', 'loss', 'down', 'weak', 'concern', 'risk']
        
        sentiment_scores = []
        relevant_news = []
        
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        
        for item in news_items:
            try:
                pub_time = datetime.fromisoformat(item['published_at'].replace('Z', '+00:00'))
                if pub_time.replace(tzinfo=None) <= cutoff_time:
                    continue
                    
                title_lower = item.get('title', '').lower()
                summary_lower = item.get('summary', '').lower()
                text = f"{title_lower} {summary_lower}"
                
                # è®¡ç®—æƒ…ç»ªå¾—åˆ†
                pos_count = sum(1 for word in positive_words if word in text)
                neg_count = sum(1 for word in negative_words if word in text)
                
                if pos_count > 0 or neg_count > 0:
                    score = (pos_count - neg_count) / (pos_count + neg_count + 1)
                    sentiment_scores.append(score)
                    relevant_news.append(item)
                    
            except:
                continue
        
        if not sentiment_scores:
            return {'sentiment': 'neutral', 'confidence': 0, 'news_count': 0}
        
        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)
        
        if avg_sentiment > 0.2:
            sentiment = 'positive'
        elif avg_sentiment < -0.2:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        return {
            'sentiment': sentiment,
            'score': avg_sentiment,
            'confidence': min(len(sentiment_scores) / 10, 1.0),
            'news_count': len(relevant_news),
            'sample_news': relevant_news[:3]
        }

    def _format_sentiment_response(self, analysis: Dict, hours: int, focus: Optional[str]) -> str:
        """æ ¼å¼åŒ–æƒ…ç»ªåˆ†æå“åº”"""
        sentiment_emoji = {
            'positive': 'ğŸ“ˆ ä¹è§‚',
            'negative': 'ğŸ“‰ æ‚²è§‚', 
            'neutral': 'ğŸ“Š ä¸­æ€§'
        }
        
        sentiment_text = sentiment_emoji.get(analysis['sentiment'], 'â“ æœªçŸ¥')
        confidence = analysis['confidence'] * 100
        
        lines = [
            f"ğŸ¯ å¸‚åœºæƒ…ç»ªåˆ†ææŠ¥å‘Š",
            f"ğŸ“… æ—¶é—´èŒƒå›´: æœ€è¿‘ {hours} å°æ—¶"
        ]
        
        if focus:
            lines.append(f"ğŸ” å…³æ³¨é¢†åŸŸ: {focus}")
        
        lines.extend([
            f"ğŸ“Š æ•´ä½“æƒ…ç»ª: {sentiment_text}",
            f"ğŸšï¸  ç½®ä¿¡åº¦: {confidence:.0f}%",
            f"ğŸ“° åˆ†ææ–°é—»: {analysis['news_count']} æ¡",
            ""
        ])
        
        if analysis.get('sample_news'):
            lines.append("ğŸ“‹ ä»£è¡¨æ€§æ–°é—»:")
            for i, news in enumerate(analysis['sample_news'], 1):
                title = self.translator.translate_title(news.get('title', ''))
                lines.append(f"{i}. {title}")
            lines.append("")
        
        # æŠ•èµ„å»ºè®®
        if analysis['sentiment'] == 'positive' and confidence > 60:
            lines.append("ğŸ’¡ æŠ•èµ„å»ºè®®: å¸‚åœºæƒ…ç»ªåä¹è§‚ï¼Œå¯è€ƒè™‘é€‚åº¦å¢åŠ ä»“ä½ï¼Œä½†éœ€æ³¨æ„é£é™©æ§åˆ¶ã€‚")
        elif analysis['sentiment'] == 'negative' and confidence > 60:
            lines.append("âš ï¸  æŠ•èµ„å»ºè®®: å¸‚åœºæƒ…ç»ªåæ‚²è§‚ï¼Œå»ºè®®ä¿æŒè°¨æ…ï¼Œå¯è€ƒè™‘å‡ä»“æˆ–è§‚æœ›ã€‚")
        else:
            lines.append("ğŸ¤” æŠ•èµ„å»ºè®®: å¸‚åœºæƒ…ç»ªä¸­æ€§ï¼Œå»ºè®®ä¿æŒç°æœ‰ç­–ç•¥ï¼Œå…³æ³¨å…·ä½“ä¸ªè‚¡æœºä¼šã€‚")
        
        return "\n".join(lines)

    def _arun(self, **kwargs):
        raise NotImplementedError("å¼‚æ­¥æš‚ä¸æ”¯æŒ")
